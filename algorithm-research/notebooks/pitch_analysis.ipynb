{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pitch Detection Algorithm Analysis\n",
    "\n",
    "This notebook helps you analyze audio samples and test different pitch detection algorithms.\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, make sure you've installed the required packages:\n",
    "```bash\n",
    "pip install -r ../requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import signal\n",
    "from scipy.fft import fft, fftfreq\n",
    "import soundfile as sf\n",
    "from pathlib import Path\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Audio Sample\n",
    "\n",
    "Load a recorded audio sample from the samples folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List available samples\n",
    "samples_dir = Path('../samples')\n",
    "audio_files = list(samples_dir.rglob('*.webm')) + list(samples_dir.rglob('*.wav')) + list(samples_dir.rglob('*.mp3'))\n",
    "\n",
    "print(\"Available audio samples:\")\n",
    "for i, file in enumerate(audio_files):\n",
    "    print(f\"{i}: {file.relative_to(samples_dir)}\")\n",
    "\n",
    "# If no files found\n",
    "if not audio_files:\n",
    "    print(\"\\nNo audio samples found. Please record some using the Sound2Score app!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a specific audio file\n",
    "# Change the index or path to load different files\n",
    "if audio_files:\n",
    "    audio_path = audio_files[0]  # Change index to load different file\n",
    "    \n",
    "    # Load audio with librosa\n",
    "    y, sr = librosa.load(audio_path, sr=None)\n",
    "    \n",
    "    print(f\"Loaded: {audio_path.name}\")\n",
    "    print(f\"Sample rate: {sr} Hz\")\n",
    "    print(f\"Duration: {len(y) / sr:.2f} seconds\")\n",
    "    print(f\"Samples: {len(y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Visualize Audio Signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if audio_files:\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(14, 10))\n",
    "    \n",
    "    # Waveform\n",
    "    librosa.display.waveshow(y, sr=sr, ax=axes[0])\n",
    "    axes[0].set_title('Waveform')\n",
    "    axes[0].set_xlabel('Time (s)')\n",
    "    axes[0].set_ylabel('Amplitude')\n",
    "    \n",
    "    # Spectrogram\n",
    "    D = librosa.amplitude_to_db(np.abs(librosa.stft(y)), ref=np.max)\n",
    "    img = librosa.display.specshow(D, sr=sr, x_axis='time', y_axis='hz', ax=axes[1])\n",
    "    axes[1].set_title('Spectrogram')\n",
    "    axes[1].set_ylim([0, 4000])  # Focus on piano frequency range\n",
    "    fig.colorbar(img, ax=axes[1], format='%+2.0f dB')\n",
    "    \n",
    "    # Chromagram (shows musical notes over time)\n",
    "    chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n",
    "    img2 = librosa.display.specshow(chroma, sr=sr, x_axis='time', y_axis='chroma', ax=axes[2])\n",
    "    axes[2].set_title('Chromagram')\n",
    "    fig.colorbar(img2, ax=axes[2])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pitch Detection Methods\n",
    "\n",
    "### Method 1: Autocorrelation (Current JS Implementation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autocorrelation_pitch(audio_buffer, sample_rate, min_freq=27.5, max_freq=4186):\n",
    "    \"\"\"\n",
    "    Autocorrelation-based pitch detection (similar to current JS implementation)\n",
    "    \"\"\"\n",
    "    # Calculate RMS\n",
    "    rms = np.sqrt(np.mean(audio_buffer ** 2))\n",
    "    if rms < 0.01:\n",
    "        return None\n",
    "    \n",
    "    # Autocorrelation\n",
    "    size = len(audio_buffer)\n",
    "    max_samples = size // 2\n",
    "    \n",
    "    correlation = np.correlate(audio_buffer, audio_buffer, mode='full')\n",
    "    correlation = correlation[size-1:]\n",
    "    \n",
    "    # Normalize\n",
    "    correlation = correlation / correlation[0]\n",
    "    \n",
    "    # Find first peak after zero\n",
    "    min_period = int(sample_rate / max_freq)\n",
    "    max_period = int(sample_rate / min_freq)\n",
    "    \n",
    "    # Search for peak\n",
    "    peak_idx = np.argmax(correlation[min_period:max_period]) + min_period\n",
    "    \n",
    "    if correlation[peak_idx] > 0.5:  # Threshold for confidence\n",
    "        frequency = sample_rate / peak_idx\n",
    "        return frequency\n",
    "    \n",
    "    return None\n",
    "\n",
    "if audio_files:\n",
    "    # Test on a small window\n",
    "    window_size = 4096\n",
    "    test_window = y[:window_size]\n",
    "    \n",
    "    freq = autocorrelation_pitch(test_window, sr)\n",
    "    if freq:\n",
    "        print(f\"Detected frequency (Autocorrelation): {freq:.2f} Hz\")\n",
    "        # Convert to note\n",
    "        note = librosa.hz_to_note(freq)\n",
    "        print(f\"Note: {note}\")\n",
    "    else:\n",
    "        print(\"No pitch detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: YIN Algorithm (Current Python Backend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yin_pitch(audio_buffer, sample_rate, threshold=0.1):\n",
    "    \"\"\"\n",
    "    YIN algorithm for pitch detection\n",
    "    \"\"\"\n",
    "    buffer_size = len(audio_buffer)\n",
    "    half_size = buffer_size // 2\n",
    "    \n",
    "    # Step 1: Calculate difference function\n",
    "    diff = np.zeros(half_size)\n",
    "    for tau in range(1, half_size):\n",
    "        for i in range(half_size):\n",
    "            diff[tau] += (audio_buffer[i] - audio_buffer[i + tau]) ** 2\n",
    "    \n",
    "    # Step 2: Cumulative mean normalized difference\n",
    "    cmnd = np.zeros(half_size)\n",
    "    cmnd[0] = 1\n",
    "    running_sum = 0\n",
    "    for tau in range(1, half_size):\n",
    "        running_sum += diff[tau]\n",
    "        cmnd[tau] = diff[tau] / (running_sum / tau)\n",
    "    \n",
    "    # Step 3: Find first minimum below threshold\n",
    "    tau = 1\n",
    "    while tau < half_size:\n",
    "        if cmnd[tau] < threshold:\n",
    "            while tau + 1 < half_size and cmnd[tau + 1] < cmnd[tau]:\n",
    "                tau += 1\n",
    "            frequency = sample_rate / tau\n",
    "            return frequency\n",
    "        tau += 1\n",
    "    \n",
    "    return None\n",
    "\n",
    "if audio_files:\n",
    "    freq = yin_pitch(test_window, sr)\n",
    "    if freq:\n",
    "        print(f\"Detected frequency (YIN): {freq:.2f} Hz\")\n",
    "        note = librosa.hz_to_note(freq)\n",
    "        print(f\"Note: {note}\")\n",
    "    else:\n",
    "        print(\"No pitch detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 3: FFT-based Harmonic Product Spectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hps_pitch(audio_buffer, sample_rate, num_harmonics=5):\n",
    "    \"\"\"\n",
    "    Harmonic Product Spectrum method\n",
    "    \"\"\"\n",
    "    # Compute FFT\n",
    "    fft_data = np.abs(fft(audio_buffer))\n",
    "    freqs = fftfreq(len(audio_buffer), 1/sample_rate)\n",
    "    \n",
    "    # Only use positive frequencies\n",
    "    positive_freqs = freqs[:len(freqs)//2]\n",
    "    positive_fft = fft_data[:len(fft_data)//2]\n",
    "    \n",
    "    # Initialize HPS with the spectrum\n",
    "    hps = positive_fft.copy()\n",
    "    \n",
    "    # Multiply downsampled versions\n",
    "    for h in range(2, num_harmonics + 1):\n",
    "        decimated = positive_fft[::h]\n",
    "        hps[:len(decimated)] *= decimated\n",
    "    \n",
    "    # Find peak\n",
    "    peak_idx = np.argmax(hps)\n",
    "    frequency = positive_freqs[peak_idx]\n",
    "    \n",
    "    if frequency > 27.5 and frequency < 4186:\n",
    "        return frequency\n",
    "    return None\n",
    "\n",
    "if audio_files:\n",
    "    freq = hps_pitch(test_window, sr)\n",
    "    if freq:\n",
    "        print(f\"Detected frequency (HPS): {freq:.2f} Hz\")\n",
    "        note = librosa.hz_to_note(freq)\n",
    "        print(f\"Note: {note}\")\n",
    "    else:\n",
    "        print(\"No pitch detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 4: Using librosa's piptrack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if audio_files:\n",
    "    # Extract pitch using librosa\n",
    "    pitches, magnitudes = librosa.piptrack(y=y, sr=sr, fmin=27.5, fmax=4186)\n",
    "    \n",
    "    # Get the pitch with highest magnitude in first frame\n",
    "    pitch_idx = magnitudes[:, 0].argmax()\n",
    "    pitch = pitches[pitch_idx, 0]\n",
    "    \n",
    "    if pitch > 0:\n",
    "        print(f\"Detected frequency (librosa piptrack): {pitch:.2f} Hz\")\n",
    "        note = librosa.hz_to_note(pitch)\n",
    "        print(f\"Note: {note}\")\n",
    "    else:\n",
    "        print(\"No pitch detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compare All Methods on Full Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if audio_files:\n",
    "    # Analyze audio in frames\n",
    "    frame_size = 4096\n",
    "    hop_length = 2048\n",
    "    \n",
    "    results = {\n",
    "        'autocorrelation': [],\n",
    "        'yin': [],\n",
    "        'hps': [],\n",
    "        'times': []\n",
    "    }\n",
    "    \n",
    "    for i in range(0, len(y) - frame_size, hop_length):\n",
    "        frame = y[i:i+frame_size]\n",
    "        time = i / sr\n",
    "        \n",
    "        results['times'].append(time)\n",
    "        results['autocorrelation'].append(autocorrelation_pitch(frame, sr))\n",
    "        results['yin'].append(yin_pitch(frame, sr))\n",
    "        results['hps'].append(hps_pitch(frame, sr))\n",
    "    \n",
    "    # Plot comparison\n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "    \n",
    "    for method in ['autocorrelation', 'yin', 'hps']:\n",
    "        freqs = [f if f else np.nan for f in results[method]]\n",
    "        ax.plot(results['times'], freqs, label=method.upper(), marker='o', markersize=3, alpha=0.7)\n",
    "    \n",
    "    ax.set_xlabel('Time (s)')\n",
    "    ax.set_ylabel('Frequency (Hz)')\n",
    "    ax.set_title('Pitch Detection Methods Comparison')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nMethod Statistics:\")\n",
    "    for method in ['autocorrelation', 'yin', 'hps']:\n",
    "        valid_detections = [f for f in results[method] if f is not None]\n",
    "        detection_rate = len(valid_detections) / len(results[method]) * 100\n",
    "        print(f\"{method.upper()}: {detection_rate:.1f}% detection rate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyze Specific Segment\n",
    "\n",
    "Pick a time segment to analyze in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if audio_files:\n",
    "    # Select time range (in seconds)\n",
    "    start_time = 0.5\n",
    "    end_time = 1.5\n",
    "    \n",
    "    start_sample = int(start_time * sr)\n",
    "    end_sample = int(end_time * sr)\n",
    "    segment = y[start_sample:end_sample]\n",
    "    \n",
    "    # Visualize segment\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(14, 8))\n",
    "    \n",
    "    # Time domain\n",
    "    time_axis = np.linspace(start_time, end_time, len(segment))\n",
    "    axes[0].plot(time_axis, segment)\n",
    "    axes[0].set_title('Audio Segment (Time Domain)')\n",
    "    axes[0].set_xlabel('Time (s)')\n",
    "    axes[0].set_ylabel('Amplitude')\n",
    "    \n",
    "    # Frequency domain\n",
    "    fft_data = np.abs(fft(segment))\n",
    "    freqs = fftfreq(len(segment), 1/sr)\n",
    "    \n",
    "    # Plot only positive frequencies up to 2000 Hz\n",
    "    positive_mask = (freqs > 0) & (freqs < 2000)\n",
    "    axes[1].plot(freqs[positive_mask], fft_data[positive_mask])\n",
    "    axes[1].set_title('Frequency Spectrum')\n",
    "    axes[1].set_xlabel('Frequency (Hz)')\n",
    "    axes[1].set_ylabel('Magnitude')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Detect pitch in segment\n",
    "    print(\"\\nPitch Detection Results for Segment:\")\n",
    "    for method_name, method_func in [('Autocorrelation', autocorrelation_pitch), \n",
    "                                       ('YIN', yin_pitch), \n",
    "                                       ('HPS', hps_pitch)]:\n",
    "        freq = method_func(segment, sr)\n",
    "        if freq:\n",
    "            note = librosa.hz_to_note(freq)\n",
    "            print(f\"{method_name:15s}: {freq:7.2f} Hz -> {note}\")\n",
    "        else:\n",
    "            print(f\"{method_name:15s}: No pitch detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Your Findings\n",
    "\n",
    "Document what you learned and save visualizations to the results folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save your notes and findings\n",
    "notes = \"\"\"\n",
    "Experiment Date: [DATE]\n",
    "Audio File: [FILENAME]\n",
    "\n",
    "Findings:\n",
    "- \n",
    "- \n",
    "\n",
    "Best performing method:\n",
    "- \n",
    "\n",
    "Next steps:\n",
    "- \n",
    "\"\"\"\n",
    "\n",
    "# Uncomment to save\n",
    "# with open('../results/experiment_notes.txt', 'a') as f:\n",
    "#     f.write(notes)\n",
    "\n",
    "print(\"Remember to document your findings!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
